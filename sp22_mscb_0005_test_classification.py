# -*- coding: utf-8 -*-
"""SP22-MSCB-0005-Test-Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19he46RXbRg6FDxu8SL5MZ1TKtTY6jtmB
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("lakshmi25npathi/imdb-dataset-of-50k-movie-reviews")

print("Path to dataset files:", path)

import os
import pandas as pd

# The path to the dataset folder
dataset_folder = "/root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1"

# List all files in the dataset folder to confirm the file name
files = os.listdir(dataset_folder)
print("Files in the dataset folder:", files)

# Assuming the CSV file is named "IMDB Dataset.csv"
csv_file = os.path.join(dataset_folder, "IMDB Dataset.csv")

# Load the CSV file into a DataFrame without any text pre-processing
df = pd.read_csv(csv_file)

# Display the first few rows to verify the contents
print(df.head())

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the dataset from the CSV file
df = pd.read_csv(csv_file)

# Inspect the first few rows to confirm the structure
print(df.head())

# Optionally, if your algorithm requires numerical labels, you can map the sentiment
# This step is minimal and only changes the label format, not the raw text
df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})

# If you need to split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'],
                                                    test_size=0.2, random_state=42)

print("Training set size:", X_train.shape[0])
print("Test set size:", X_test.shape[0])

import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Naive Byers
# -----------------------
# 2. Split the Data
# -----------------------

# Use an 80/20 train-test split
X_train, X_test, y_train, y_test = train_test_split(
    df['review'], df['sentiment'], test_size=0.2, random_state=42
)

# -----------------------
# 3. Vectorize the Text (No Additional Pre-processing)
# -----------------------

# Here we use CountVectorizer with lowercase=False to avoid lowercasing.
# We are not removing stop words or performing any other pre-processing.
vectorizer = CountVectorizer(lowercase=False)
X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# -----------------------
# 4. Define and Train Classifiers
# -----------------------

# Three classifiers: Naïve Bayes, Logistic Regression, and Linear SVM
classifiers = {
    "Naive Bayes": MultinomialNB()
}

# -----------------------
# 5. Evaluate and Publish Results
# -----------------------

for clf_name, clf in classifiers.items():
    print(f"--- {clf_name} ---")

    # Train the classifier on the training data
    clf.fit(X_train_vect, y_train)

    # Predict on the test data
    y_pred = clf.predict(X_test_vect)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    # Print out the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(report)
    print("Confusion Matrix:")
    print(cm)
    print("\n")

# Logistic Regression
# -----------------------
# 2. Split the Data
# -----------------------

# Use an 80/20 train-test split
X_train, X_test, y_train, y_test = train_test_split(
    df['review'], df['sentiment'], test_size=0.2, random_state=42
)

# -----------------------
# 3. Vectorize the Text (No Additional Pre-processing)
# -----------------------

# Here we use CountVectorizer with lowercase=False to avoid lowercasing.
# We are not removing stop words or performing any other pre-processing.
vectorizer = CountVectorizer(lowercase=False)
X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# -----------------------
# 4. Define and Train Classifiers
# -----------------------

# Three classifiers: Naïve Bayes, Logistic Regression, and Linear SVM
classifiers = {
    "Logistic Regression": LogisticRegression(max_iter=1000)
}

# -----------------------
# 5. Evaluate and Publish Results
# -----------------------

for clf_name, clf in classifiers.items():
    print(f"--- {clf_name} ---")

    # Train the classifier on the training data
    clf.fit(X_train_vect, y_train)

    # Predict on the test data
    y_pred = clf.predict(X_test_vect)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    # Print out the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(report)
    print("Confusion Matrix:")
    print(cm)
    print("\n")

# SVM
# -----------------------
# 2. Split the Data
# -----------------------

# Use an 80/20 train-test split
X_train, X_test, y_train, y_test = train_test_split(
    df['review'], df['sentiment'], test_size=0.2, random_state=42
)

# -----------------------
# 3. Vectorize the Text (No Additional Pre-processing)
# -----------------------

# Here we use CountVectorizer with lowercase=False to avoid lowercasing.
# We are not removing stop words or performing any other pre-processing.
vectorizer = CountVectorizer(lowercase=False)
X_train_vect = vectorizer.fit_transform(X_train)
X_test_vect = vectorizer.transform(X_test)

# -----------------------
# 4. Define and Train Classifiers
# -----------------------

# Three classifiers: Naïve Bayes, Logistic Regression, and Linear SVM
classifiers = {
    "SVM": LinearSVC(max_iter=5000)
}

# -----------------------
# 5. Evaluate and Publish Results
# -----------------------

for clf_name, clf in classifiers.items():
    print(f"--- {clf_name} ---")

    # Train the classifier on the training data
    clf.fit(X_train_vect, y_train)

    # Predict on the test data
    y_pred = clf.predict(X_test_vect)

    # Compute evaluation metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)

    # Print out the results
    print(f"Accuracy: {accuracy:.4f}")
    print("Classification Report:")
    print(report)
    print("Confusion Matrix:")
    print(cm)
    print("\n")

# -----------------------
# Preprocessing Functions & Settings
# -----------------------
# Custom preprocessor to remove punctuation
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Create a custom stop words list.
# Convert to list because CountVectorizer expects a list (or string 'english')
custom_stop_words = list(ENGLISH_STOP_WORDS.union({word.capitalize() for word in ENGLISH_STOP_WORDS}))

# -----------------------
# 3. Define Experiments with Different Settings
# -----------------------
experiments = {
    "Baseline (No additional processing)": CountVectorizer(lowercase=False),
    "Remove Stop Words and Punctuation": CountVectorizer(lowercase=False,
                                                        preprocessor=remove_punctuation,
                                                        stop_words=custom_stop_words),
    "Lowercase": CountVectorizer(lowercase=True),  # default lowercasing
    "TF-IDF": TfidfVectorizer(lowercase=False)       # using TF-IDF features; can set lowercase=True if desired
}

# -----------------------
# 4. Define Classifiers
# -----------------------
classifiers = {
    "Naive Bayes": MultinomialNB(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": LinearSVC(max_iter=10000)  # Increased iterations to help convergence
}

# -----------------------
# 5. Run Experiments and Evaluate
# -----------------------
results = {}  # to store results for comparison

for exp_name, vectorizer in experiments.items():
    print(f"=== Experiment: {exp_name} ===\n")

    # Vectorize training and test data using the current experiment's settings
    X_train_vect = vectorizer.fit_transform(X_train)
    X_test_vect = vectorizer.transform(X_test)

    results[exp_name] = {}

    for clf_name, clf in classifiers.items():
        # Train classifier
        clf.fit(X_train_vect, y_train)
        # Predict test set
        y_pred = clf.predict(X_test_vect)

        # Evaluate predictions
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred)
        cm = confusion_matrix(y_test, y_pred)

        # Store results
        results[exp_name][clf_name] = {
            "accuracy": accuracy,
            "classification_report": report,
            "confusion_matrix": cm
        }

        # Print results for this classifier
        print(f"--- {clf_name} ---")
        print(f"Accuracy: {accuracy:.4f}")
        print("Classification Report:")
        print(report)
        print("Confusion Matrix:")
        print(cm)
        print("\n")
    print("\n\n")

"""Within-Algorithm Comparisons

Naïve Bayes

**Baseline (No Processing)**:
Accuracy is 85.02% with balanced precision and recall. The confusion matrix indicates moderate misclassification with 592 false positives (FP) and 906 false negatives (FN).

**Remove Stop Words and Punctuation**:
Accuracy improves to 86.04%. The reduction in FNs (from 906 to 787) suggests that eliminating punctuation and stop words helps Naïve Bayes by reducing noise.

**Lowercase**:
Accuracy slightly drops to 84.88%. The confusion matrix shows an increase in FN (from 906 to 912), indicating that the original casing might carry some useful signals for this probabilistic model.

**TF-IDF**:
Accuracy rises to 86.42%. The feature weighting emphasizes more informative words, leading to a better balance in precision and recall (FNs reduced to 807 and a slightly improved FP count), which is beneficial for a model that relies on word frequency.


**Logistic Regression**

**Baseline**:
Accuracy is 88.96% with strong performance on both classes. The model misclassifies 587 instances for class 0 and 517 for class 1.

**Remove Stop Words and Punctuation**:
Accuracy slightly drops to 88.59%. Although the confusion matrix shows a similar distribution (FP and FN values are close to baseline), the removal of some tokens might have removed subtle cues.

**Lowercase**:
Accuracy is nearly unchanged at 88.74%. This suggests that for Logistic Regression, simply converting to lowercase does not significantly alter the feature space when compared to the unaltered text.

**TF-IDF**:
The most significant boost is observed here, with accuracy at 90.09%. The confusion matrix improves notably with fewer misclassifications (534 FP and 457 FN), indicating that TF-IDF’s ability to down-weight common words enhances the discriminative power of the features.

**SVM**

**Baseline**:
Accuracy stands at 87.41%. The model shows relatively balanced performance, although the convergence warning hints that more iterations might be beneficial.

**Remove Stop Words and Punctuation**:
Accuracy remains similar at 87.10%. The minor changes in the confusion matrix indicate that SVM is less sensitive to these specific pre-processing steps.

**Lowercase**:
Accuracy dips slightly to 86.64%. An increase in both FP and FN suggests that preserving case might be contributing useful information for the SVM decision boundary.

**TF-IDF**:
Accuracy peaks at 90.47%. With the lowest FP (518) and FN (435) counts among all experiments, TF-IDF clearly benefits SVM by providing a more refined feature representation that better captures the underlying differences between classes.

⸻

Between-Algorithm Comparisons

**Naïve Bayes vs. Others**:
Across all settings, Naïve Bayes consistently underperforms compared to Logistic Regression and SVM. Its independence assumption may be too simplistic for capturing the nuances in movie reviews. However, when using TF-IDF, its performance improves, highlighting that even simple models benefit from better feature weighting.

**Logistic Regression**:
This model shows robust performance across different settings. It reaches its highest accuracy with TF-IDF, suggesting that Logistic Regression is effective at leveraging weighted features. The model is relatively stable when applying or omitting pre‑processing steps like lowercasing or stop word removal.

**SVM**:
SVM demonstrates the most significant gains when using TF-IDF, achieving the highest accuracy overall (90.47%). This indicates that SVM is particularly sensitive to feature quality. While it shows modest declines when removing stop words or lowercasing, the clear benefit from TF-IDF suggests that providing well-scaled features allows SVM to better define the optimal decision boundary.

⸻

Overall Insights

Pre‑processing Effects:

Stop Words and Punctuation Removal: Benefits Naïve Bayes slightly by reducing
noise, though its impact on Logistic Regression and SVM is marginal.

Lowercasing: Has a small negative effect on Naïve Bayes and SVM, possibly because case information (such as proper names or emphatic words) can be informative.

TF-IDF: Provides the most consistent improvement across all classifiers by weighting features according to their discriminative power. Both Logistic Regression and SVM show a notable increase in accuracy, while Naïve Bayes also benefits modestly.


Algorithm Sensitivity:

Naïve Bayes is more affected by noisy features, so any pre‑processing that reduces noise (like TF-IDF) helps, but its overall simplicity limits its performance.

Logistic Regression performs robustly across various settings, and benefits from the enhanced feature representation offered by TF-IDF.

SVM achieves its best results with TF-IDF, indicating that margin-based classifiers can extract more value from nuanced, weighted features.

"""