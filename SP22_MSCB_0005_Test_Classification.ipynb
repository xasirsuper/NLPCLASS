{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ETJh7MC9KUU",
        "outputId": "f26acc29-1ea9-4f6c-a6aa-79342ea2e6af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 25.7M/25.7M [00:00<00:00, 157MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# The path to the dataset folder\n",
        "dataset_folder = \"/root/.cache/kagglehub/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews/versions/1\"\n",
        "\n",
        "# List all files in the dataset folder to confirm the file name\n",
        "files = os.listdir(dataset_folder)\n",
        "print(\"Files in the dataset folder:\", files)\n",
        "\n",
        "# Assuming the CSV file is named \"IMDB Dataset.csv\"\n",
        "csv_file = os.path.join(dataset_folder, \"IMDB Dataset.csv\")\n",
        "\n",
        "# Load the CSV file into a DataFrame without any text pre-processing\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Display the first few rows to verify the contents\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ssdz4bs6--Df",
        "outputId": "05213f01-e313-40b8-c98f-12b288031b51"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in the dataset folder: ['IMDB Dataset.csv']\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the dataset from the CSV file\n",
        "df = pd.read_csv(csv_file)\n",
        "\n",
        "# Inspect the first few rows to confirm the structure\n",
        "print(df.head())\n",
        "\n",
        "# Optionally, if your algorithm requires numerical labels, you can map the sentiment\n",
        "# This step is minimal and only changes the label format, not the raw text\n",
        "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
        "\n",
        "# If you need to split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'],\n",
        "                                                    test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training set size:\", X_train.shape[0])\n",
        "print(\"Test set size:\", X_test.shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baNdkbTc-RVl",
        "outputId": "9ec9abb0-85c8-49c1-aeab-bfe24ce03ed5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "Training set size: 40000\n",
            "Test set size: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "Zkg5HjGMFTiw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Byers\n",
        "# -----------------------\n",
        "# 2. Split the Data\n",
        "# -----------------------\n",
        "\n",
        "# Use an 80/20 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review'], df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 3. Vectorize the Text (No Additional Pre-processing)\n",
        "# -----------------------\n",
        "\n",
        "# Here we use CountVectorizer with lowercase=False to avoid lowercasing.\n",
        "# We are not removing stop words or performing any other pre-processing.\n",
        "vectorizer = CountVectorizer(lowercase=False)\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# -----------------------\n",
        "# 4. Define and Train Classifiers\n",
        "# -----------------------\n",
        "\n",
        "# Three classifiers: Naïve Bayes, Logistic Regression, and Linear SVM\n",
        "classifiers = {\n",
        "    \"Naive Bayes\": MultinomialNB()\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 5. Evaluate and Publish Results\n",
        "# -----------------------\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"--- {clf_name} ---\")\n",
        "\n",
        "    # Train the classifier on the training data\n",
        "    clf.fit(X_train_vect, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = clf.predict(X_test_vect)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print out the results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rV7GcsuoEXNq",
        "outputId": "2f7bc58a-0e56-4bbb-f2c8-b923d033830c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Naive Bayes ---\n",
            "Accuracy: 0.8502\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85      4961\n",
            "           1       0.87      0.82      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4369  592]\n",
            " [ 906 4133]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "# -----------------------\n",
        "# 2. Split the Data\n",
        "# -----------------------\n",
        "\n",
        "# Use an 80/20 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review'], df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 3. Vectorize the Text (No Additional Pre-processing)\n",
        "# -----------------------\n",
        "\n",
        "# Here we use CountVectorizer with lowercase=False to avoid lowercasing.\n",
        "# We are not removing stop words or performing any other pre-processing.\n",
        "vectorizer = CountVectorizer(lowercase=False)\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# -----------------------\n",
        "# 4. Define and Train Classifiers\n",
        "# -----------------------\n",
        "\n",
        "# Three classifiers: Naïve Bayes, Logistic Regression, and Linear SVM\n",
        "classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000)\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 5. Evaluate and Publish Results\n",
        "# -----------------------\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"--- {clf_name} ---\")\n",
        "\n",
        "    # Train the classifier on the training data\n",
        "    clf.fit(X_train_vect, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = clf.predict(X_test_vect)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print out the results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdstiqlXFjUa",
        "outputId": "ac458add-ec28-467e-a594-f27cfafe16fd"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Logistic Regression ---\n",
            "Accuracy: 0.8896\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89      4961\n",
            "           1       0.89      0.90      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4374  587]\n",
            " [ 517 4522]]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM\n",
        "# -----------------------\n",
        "# 2. Split the Data\n",
        "# -----------------------\n",
        "\n",
        "# Use an 80/20 train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['review'], df['sentiment'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -----------------------\n",
        "# 3. Vectorize the Text (No Additional Pre-processing)\n",
        "# -----------------------\n",
        "\n",
        "# Here we use CountVectorizer with lowercase=False to avoid lowercasing.\n",
        "# We are not removing stop words or performing any other pre-processing.\n",
        "vectorizer = CountVectorizer(lowercase=False)\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "# -----------------------\n",
        "# 4. Define and Train Classifiers\n",
        "# -----------------------\n",
        "\n",
        "# Three classifiers: Naïve Bayes, Logistic Regression, and Linear SVM\n",
        "classifiers = {\n",
        "    \"SVM\": LinearSVC(max_iter=5000)\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 5. Evaluate and Publish Results\n",
        "# -----------------------\n",
        "\n",
        "for clf_name, clf in classifiers.items():\n",
        "    print(f\"--- {clf_name} ---\")\n",
        "\n",
        "    # Train the classifier on the training data\n",
        "    clf.fit(X_train_vect, y_train)\n",
        "\n",
        "    # Predict on the test data\n",
        "    y_pred = clf.predict(X_test_vect)\n",
        "\n",
        "    # Compute evaluation metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Print out the results\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"Classification Report:\")\n",
        "    print(report)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mveY782PFlpH",
        "outputId": "1f97ef1b-10cc-4df2-fba4-fbe8683b05d4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SVM ---\n",
            "Accuracy: 0.8742\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87      4961\n",
            "           1       0.87      0.88      0.88      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4316  645]\n",
            " [ 613 4426]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------\n",
        "# Preprocessing Functions & Settings\n",
        "# -----------------------\n",
        "# Custom preprocessor to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Create a custom stop words list.\n",
        "# Convert to list because CountVectorizer expects a list (or string 'english')\n",
        "custom_stop_words = list(ENGLISH_STOP_WORDS.union({word.capitalize() for word in ENGLISH_STOP_WORDS}))\n",
        "\n",
        "# -----------------------\n",
        "# 3. Define Experiments with Different Settings\n",
        "# -----------------------\n",
        "experiments = {\n",
        "    \"Baseline (No additional processing)\": CountVectorizer(lowercase=False),\n",
        "    \"Remove Stop Words and Punctuation\": CountVectorizer(lowercase=False,\n",
        "                                                        preprocessor=remove_punctuation,\n",
        "                                                        stop_words=custom_stop_words),\n",
        "    \"Lowercase\": CountVectorizer(lowercase=True),  # default lowercasing\n",
        "    \"TF-IDF\": TfidfVectorizer(lowercase=False)       # using TF-IDF features; can set lowercase=True if desired\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 4. Define Classifiers\n",
        "# -----------------------\n",
        "classifiers = {\n",
        "    \"Naive Bayes\": MultinomialNB(),\n",
        "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
        "    \"SVM\": LinearSVC(max_iter=10000)  # Increased iterations to help convergence\n",
        "}\n",
        "\n",
        "# -----------------------\n",
        "# 5. Run Experiments and Evaluate\n",
        "# -----------------------\n",
        "results = {}  # to store results for comparison\n",
        "\n",
        "for exp_name, vectorizer in experiments.items():\n",
        "    print(f\"=== Experiment: {exp_name} ===\\n\")\n",
        "\n",
        "    # Vectorize training and test data using the current experiment's settings\n",
        "    X_train_vect = vectorizer.fit_transform(X_train)\n",
        "    X_test_vect = vectorizer.transform(X_test)\n",
        "\n",
        "    results[exp_name] = {}\n",
        "\n",
        "    for clf_name, clf in classifiers.items():\n",
        "        # Train classifier\n",
        "        clf.fit(X_train_vect, y_train)\n",
        "        # Predict test set\n",
        "        y_pred = clf.predict(X_test_vect)\n",
        "\n",
        "        # Evaluate predictions\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results[exp_name][clf_name] = {\n",
        "            \"accuracy\": accuracy,\n",
        "            \"classification_report\": report,\n",
        "            \"confusion_matrix\": cm\n",
        "        }\n",
        "\n",
        "        # Print results for this classifier\n",
        "        print(f\"--- {clf_name} ---\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "        print(\"Confusion Matrix:\")\n",
        "        print(cm)\n",
        "        print(\"\\n\")\n",
        "    print(\"\\n\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xvxp280BDcF",
        "outputId": "d649bac9-fbb8-4a9a-ef54-1181703f64fd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Experiment: Baseline (No additional processing) ===\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Accuracy: 0.8502\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85      4961\n",
            "           1       0.87      0.82      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4369  592]\n",
            " [ 906 4133]]\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.8896\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89      4961\n",
            "           1       0.89      0.90      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4374  587]\n",
            " [ 517 4522]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SVM ---\n",
            "Accuracy: 0.8741\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.87      0.87      4961\n",
            "           1       0.87      0.88      0.88      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4316  645]\n",
            " [ 614 4425]]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "=== Experiment: Remove Stop Words and Punctuation ===\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Accuracy: 0.8604\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86      4961\n",
            "           1       0.87      0.84      0.86      5039\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.86      0.86      0.86     10000\n",
            "weighted avg       0.86      0.86      0.86     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4352  609]\n",
            " [ 787 4252]]\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.8859\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.88      4961\n",
            "           1       0.88      0.89      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4363  598]\n",
            " [ 543 4496]]\n",
            "\n",
            "\n",
            "--- SVM ---\n",
            "Accuracy: 0.8710\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.87      0.87      4961\n",
            "           1       0.87      0.87      0.87      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4304  657]\n",
            " [ 633 4406]]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "=== Experiment: Lowercase ===\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Accuracy: 0.8488\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.88      0.85      4961\n",
            "           1       0.87      0.82      0.85      5039\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4361  600]\n",
            " [ 912 4127]]\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.8874\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89      4961\n",
            "           1       0.88      0.90      0.89      5039\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4358  603]\n",
            " [ 523 4516]]\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/svm/_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- SVM ---\n",
            "Accuracy: 0.8664\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86      4961\n",
            "           1       0.86      0.87      0.87      5039\n",
            "\n",
            "    accuracy                           0.87     10000\n",
            "   macro avg       0.87      0.87      0.87     10000\n",
            "weighted avg       0.87      0.87      0.87     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4276  685]\n",
            " [ 651 4388]]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "=== Experiment: TF-IDF ===\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Accuracy: 0.8642\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.89      0.87      4961\n",
            "           1       0.88      0.84      0.86      5039\n",
            "\n",
            "    accuracy                           0.86     10000\n",
            "   macro avg       0.87      0.86      0.86     10000\n",
            "weighted avg       0.87      0.86      0.86     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4410  551]\n",
            " [ 807 4232]]\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.9009\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.89      0.90      4961\n",
            "           1       0.90      0.91      0.90      5039\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4427  534]\n",
            " [ 457 4582]]\n",
            "\n",
            "\n",
            "--- SVM ---\n",
            "Accuracy: 0.9047\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.90      0.90      4961\n",
            "           1       0.90      0.91      0.91      5039\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4443  518]\n",
            " [ 435 4604]]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Within-Algorithm Comparisons\n",
        "\n",
        "Naïve Bayes\n",
        "\n",
        "**Baseline (No Processing)**:\n",
        "Accuracy is 85.02% with balanced precision and recall. The confusion matrix indicates moderate misclassification with 592 false positives (FP) and 906 false negatives (FN).\n",
        "\n",
        "**Remove Stop Words and Punctuation**:\n",
        "Accuracy improves to 86.04%. The reduction in FNs (from 906 to 787) suggests that eliminating punctuation and stop words helps Naïve Bayes by reducing noise.\n",
        "\n",
        "**Lowercase**:\n",
        "Accuracy slightly drops to 84.88%. The confusion matrix shows an increase in FN (from 906 to 912), indicating that the original casing might carry some useful signals for this probabilistic model.\n",
        "\n",
        "**TF-IDF**:\n",
        "Accuracy rises to 86.42%. The feature weighting emphasizes more informative words, leading to a better balance in precision and recall (FNs reduced to 807 and a slightly improved FP count), which is beneficial for a model that relies on word frequency.\n",
        "\n",
        "\n",
        "**Logistic Regression**\n",
        "\n",
        "**Baseline**:\n",
        "Accuracy is 88.96% with strong performance on both classes. The model misclassifies 587 instances for class 0 and 517 for class 1.\n",
        "\n",
        "**Remove Stop Words and Punctuation**:\n",
        "Accuracy slightly drops to 88.59%. Although the confusion matrix shows a similar distribution (FP and FN values are close to baseline), the removal of some tokens might have removed subtle cues.\n",
        "\n",
        "**Lowercase**:\n",
        "Accuracy is nearly unchanged at 88.74%. This suggests that for Logistic Regression, simply converting to lowercase does not significantly alter the feature space when compared to the unaltered text.\n",
        "\n",
        "**TF-IDF**:\n",
        "The most significant boost is observed here, with accuracy at 90.09%. The confusion matrix improves notably with fewer misclassifications (534 FP and 457 FN), indicating that TF-IDF’s ability to down-weight common words enhances the discriminative power of the features.\n",
        "\n",
        "**SVM**\n",
        "\n",
        "**Baseline**:\n",
        "Accuracy stands at 87.41%. The model shows relatively balanced performance, although the convergence warning hints that more iterations might be beneficial.\n",
        "\n",
        "**Remove Stop Words and Punctuation**:\n",
        "Accuracy remains similar at 87.10%. The minor changes in the confusion matrix indicate that SVM is less sensitive to these specific pre-processing steps.\n",
        "\n",
        "**Lowercase**:\n",
        "Accuracy dips slightly to 86.64%. An increase in both FP and FN suggests that preserving case might be contributing useful information for the SVM decision boundary.\n",
        "\n",
        "**TF-IDF**:\n",
        "Accuracy peaks at 90.47%. With the lowest FP (518) and FN (435) counts among all experiments, TF-IDF clearly benefits SVM by providing a more refined feature representation that better captures the underlying differences between classes.\n",
        "\n",
        "⸻\n",
        "\n",
        "Between-Algorithm Comparisons\n",
        "\n",
        "**Naïve Bayes vs. Others**:\n",
        "Across all settings, Naïve Bayes consistently underperforms compared to Logistic Regression and SVM. Its independence assumption may be too simplistic for capturing the nuances in movie reviews. However, when using TF-IDF, its performance improves, highlighting that even simple models benefit from better feature weighting.\n",
        "\n",
        "**Logistic Regression**:\n",
        "This model shows robust performance across different settings. It reaches its highest accuracy with TF-IDF, suggesting that Logistic Regression is effective at leveraging weighted features. The model is relatively stable when applying or omitting pre‑processing steps like lowercasing or stop word removal.\n",
        "\n",
        "**SVM**:\n",
        "SVM demonstrates the most significant gains when using TF-IDF, achieving the highest accuracy overall (90.47%). This indicates that SVM is particularly sensitive to feature quality. While it shows modest declines when removing stop words or lowercasing, the clear benefit from TF-IDF suggests that providing well-scaled features allows SVM to better define the optimal decision boundary.\n",
        "\n",
        "⸻\n",
        "\n",
        "Overall Insights\n",
        "\n",
        "Pre‑processing Effects:\n",
        "\n",
        "Stop Words and Punctuation Removal: Benefits Naïve Bayes slightly by reducing\n",
        "noise, though its impact on Logistic Regression and SVM is marginal.\n",
        "\n",
        "Lowercasing: Has a small negative effect on Naïve Bayes and SVM, possibly because case information (such as proper names or emphatic words) can be informative.\n",
        "\n",
        "TF-IDF: Provides the most consistent improvement across all classifiers by weighting features according to their discriminative power. Both Logistic Regression and SVM show a notable increase in accuracy, while Naïve Bayes also benefits modestly.\n",
        "\n",
        "\n",
        "Algorithm Sensitivity:\n",
        "\n",
        "Naïve Bayes is more affected by noisy features, so any pre‑processing that reduces noise (like TF-IDF) helps, but its overall simplicity limits its performance.\n",
        "\n",
        "Logistic Regression performs robustly across various settings, and benefits from the enhanced feature representation offered by TF-IDF.\n",
        "\n",
        "SVM achieves its best results with TF-IDF, indicating that margin-based classifiers can extract more value from nuanced, weighted features.\n"
      ],
      "metadata": {
        "id": "JnYV8K-0J72M"
      }
    }
  ]
}